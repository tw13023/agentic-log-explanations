{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d87f241a",
   "metadata": {},
   "source": [
    "# BGL Anomaly Detection Screener\n",
    "\n",
    "This notebook loads the pre-trained AllLinLog model and runs inference on the BGL test set for anomaly screening.\n",
    "\n",
    "## Requirements\n",
    "- `torch`\n",
    "- `numpy`\n",
    "- `pandas`\n",
    "- `scikit-learn`\n",
    "- `tqdm`\n",
    "- `tiktoken`\n",
    "- `linformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac605d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "from linformer import Linformer\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "LOG_FILE = \"./logs/BGL.log\"\n",
    "MODEL_PATH = \"./best_model/best_model_20250724_072857.pth\"\n",
    "WINDOWS_SIZE = 10\n",
    "STEP_SIZE = 10\n",
    "TRAIN_RATIO = 0.7\n",
    "SEED = 42\n",
    "BATCH_SIZE = 8\n",
    "MAX_TOKEN_LENGTH = 4096  # Will be updated after data loading\n",
    "\n",
    "# Model hyperparameters (must match training)\n",
    "CL100K_VOCAB_SIZE = 100264  # GPT4 BPE\n",
    "EMBEDDING_DIM = 128\n",
    "FF_HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 1\n",
    "NUM_HEADS = 4\n",
    "K = 32  # Linformer projection dimension\n",
    "DROPOUT = 0.5\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set random seed for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Random seed set to {seed}\")\n",
    "\n",
    "set_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5e20eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class LogDataset(Dataset):\n",
    "    def __init__(self, sessions):\n",
    "        self.sessions = sessions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sessions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sessions[idx]\n",
    "\n",
    "\n",
    "def load_gpt4_tokenizer():\n",
    "    \"\"\"Load the GPT-4 BPE tokenizer.\"\"\"\n",
    "    print(\"Loading cl100k_base (GPT-4) tokenizer...\")\n",
    "    return tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "\n",
    "def tokenize_and_construct_input(log_sequence, tokenizer, max_len=4096):\n",
    "    \"\"\"Tokenize log messages and construct input IDs and segment IDs.\"\"\"\n",
    "    input_ids = []\n",
    "    segment_ids = []\n",
    "\n",
    "    allowed_special = {\"<|startoftext|>\", \"<|endoftext|>\"}\n",
    "    bos_token = tokenizer.encode(\"<|startoftext|>\", allowed_special=allowed_special)[0]\n",
    "    eos_token = tokenizer.encode(\"<|endoftext|>\", allowed_special=allowed_special)[0]\n",
    "\n",
    "    for i, log in enumerate(log_sequence):\n",
    "        tokens = tokenizer.encode(log, allowed_special=allowed_special)\n",
    "        if i == 0:\n",
    "            tokens = [bos_token] + tokens\n",
    "        tokens = tokens + [eos_token]\n",
    "        input_ids.extend(tokens)\n",
    "        segment_ids.extend([i] * len(tokens))\n",
    "        input_ids = input_ids[:max_len]\n",
    "        segment_ids = segment_ids[:max_len]\n",
    "\n",
    "    return input_ids, segment_ids\n",
    "\n",
    "\n",
    "def create_sessions_with_segment_ids(log_data, tokenizer, windows_size, step_size):\n",
    "    \"\"\"Process log data into sessions with input IDs and segment IDs.\"\"\"\n",
    "    sessions = []\n",
    "    print(\"Creating sessions...\")\n",
    "    for i in tqdm(range(0, len(log_data) - windows_size, step_size), desc=\"Processing Sessions\"):\n",
    "        logs_in_session = []\n",
    "        label = 0\n",
    "        for j in range(i, i + windows_size):\n",
    "            content = log_data[j]\n",
    "            if content[0] != \"-\":\n",
    "                label = 1\n",
    "            content = content[content.find(' ') + 1:]\n",
    "            logs_in_session.append(content)\n",
    "\n",
    "        input_ids, segment_ids = tokenize_and_construct_input(logs_in_session, tokenizer)\n",
    "        sessions.append({\n",
    "            \"input_ids\": input_ids,\n",
    "            \"segment_ids\": segment_ids,\n",
    "            \"session_label\": label\n",
    "        })\n",
    "\n",
    "    return sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf9b2648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, segment_vocab_size, embedding_dim=128):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.segment_embedding = nn.Embedding(segment_vocab_size, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, position_ids=None):\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(input_ids.size(1), device=input_ids.device).unsqueeze(0).repeat(input_ids.size(0), 1)\n",
    "        E_token = self.token_embedding(input_ids)\n",
    "        E_segment = self.segment_embedding(segment_ids)\n",
    "        E_position = self.position_embedding(position_ids)\n",
    "        return E_token + E_segment + E_position\n",
    "\n",
    "\n",
    "class LinformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_seq_len, num_heads=2, ff_hidden_dim=128, k=128, dropout=0.1):\n",
    "        super(LinformerEncoderLayer, self).__init__()\n",
    "        self.self_attention = Linformer(\n",
    "            dim=embedding_dim,\n",
    "            seq_len=max_seq_len,\n",
    "            depth=1,\n",
    "            heads=num_heads,\n",
    "            k=k,\n",
    "            one_kv_head=True,\n",
    "            share_kv=True\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, ff_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_dim, embedding_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_output = self.self_attention(x)\n",
    "        x = self.norm1(x + self.dropout(attention_output))\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))\n",
    "        return x\n",
    "\n",
    "\n",
    "class LinformerTransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, embedding_dim, max_seq_len, num_heads=2, ff_hidden_dim=128, k=128, dropout=0.1):\n",
    "        super(LinformerTransformerEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            LinformerEncoderLayer(embedding_dim, max_seq_len, num_heads, ff_hidden_dim, k, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AllLinLog(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, segment_vocab_size, embedding_dim=128,\n",
    "                 num_layers=1, num_heads=2, ff_hidden_dim=128, k=128, num_classes=2, dropout=0.1):\n",
    "        super(AllLinLog, self).__init__()\n",
    "        self.embedding_layer = EmbeddingLayer(vocab_size, max_seq_len, segment_vocab_size, embedding_dim)\n",
    "        self.encoder = LinformerTransformerEncoder(num_layers, embedding_dim, max_seq_len, num_heads, ff_hidden_dim, k, dropout)\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, position_ids, attention_mask=None):\n",
    "        embeddings = self.embedding_layer(input_ids, segment_ids, position_ids)\n",
    "        encoder_output = self.encoder(embeddings)\n",
    "        pooled_output = torch.mean(encoder_output, dim=1)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3cce719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading logs from: ./logs/BGL.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Logs: 4747963it [00:01, 3223490.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4747963 logs in 1.48 seconds.\n",
      "Loading cl100k_base (GPT-4) tokenizer...\n",
      "Creating sessions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sessions: 100%|██████████| 474796/474796 [01:42<00:00, 4630.21it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max tokens in sessions: 2549\n",
      "\n",
      "Dataset Split:\n",
      "Train sessions: 332357 | Val sessions: 71219 | Test sessions: 71220\n",
      "\n",
      "Test set => Normal: 65366 | Anomalous: 5854\n",
      "Anomalous ratio: 8.22%\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data (same split as training for reproducibility)\n",
    "print(\"Loading logs from:\", LOG_FILE)\n",
    "start_time = time.time()\n",
    "\n",
    "with open(LOG_FILE, mode=\"r\", encoding='utf8') as f:\n",
    "    logs = [x.strip() for x in tqdm(f, desc=\"Reading Logs\")]\n",
    "\n",
    "print(f\"Loaded {len(logs)} logs in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "tokenizer = load_gpt4_tokenizer()\n",
    "all_sessions = create_sessions_with_segment_ids(logs, tokenizer, WINDOWS_SIZE, STEP_SIZE)\n",
    "\n",
    "# Calculate max token length\n",
    "token_lengths = [len(session[\"input_ids\"]) for session in all_sessions]\n",
    "MAX_TOKEN_LENGTH = max(token_lengths)\n",
    "print(f\"Max tokens in sessions: {MAX_TOKEN_LENGTH}\")\n",
    "\n",
    "# Perform the same stratified split as training\n",
    "session_labels = [s[\"session_label\"] for s in all_sessions]\n",
    "\n",
    "train_sessions, temp_sessions = train_test_split(\n",
    "    all_sessions,\n",
    "    test_size=(1 - TRAIN_RATIO),\n",
    "    stratify=session_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_sessions, test_sessions = train_test_split(\n",
    "    temp_sessions,\n",
    "    test_size=0.5,\n",
    "    stratify=[s[\"session_label\"] for s in temp_sessions],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset Split:\")\n",
    "print(f\"Train sessions: {len(train_sessions)} | Val sessions: {len(val_sessions)} | Test sessions: {len(test_sessions)}\")\n",
    "\n",
    "# Test set statistics\n",
    "test_normal = sum(s['session_label'] == 0 for s in test_sessions)\n",
    "test_anomalous = sum(s['session_label'] == 1 for s in test_sessions)\n",
    "print(f\"\\nTest set => Normal: {test_normal} | Anomalous: {test_anomalous}\")\n",
    "print(f\"Anomalous ratio: {test_anomalous/(test_anomalous + test_normal):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f44dc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test DataLoader created with 8903 batches\n"
     ]
    }
   ],
   "source": [
    "# Collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    input_ids = [torch.tensor(item[\"input_ids\"], dtype=torch.long) for item in batch]\n",
    "    segment_ids = [torch.tensor(item[\"segment_ids\"], dtype=torch.long) for item in batch]\n",
    "    session_labels = torch.tensor([item[\"session_label\"] for item in batch], dtype=torch.long)\n",
    "\n",
    "    padded_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    padded_segment_ids = pad_sequence(segment_ids, batch_first=True, padding_value=0)\n",
    "\n",
    "    padded_input_ids = padded_input_ids[:, :MAX_TOKEN_LENGTH]\n",
    "    padded_segment_ids = padded_segment_ids[:, :MAX_TOKEN_LENGTH]\n",
    "\n",
    "    attention_masks = (padded_input_ids != 0).long()\n",
    "\n",
    "    return padded_input_ids, padded_segment_ids, attention_masks, session_labels\n",
    "\n",
    "\n",
    "# Create test dataloader\n",
    "test_dataset = LogDataset(test_sessions)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Test DataLoader created with {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d0abef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: ./best_model/best_model_20250724_072857.pth\n",
      "Model loaded successfully!\n",
      "Total parameters: 13,445,922\n",
      "Model size: 51.29 MB\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model\n",
    "print(f\"Loading model from: {MODEL_PATH}\")\n",
    "\n",
    "model = AllLinLog(\n",
    "    vocab_size=CL100K_VOCAB_SIZE,\n",
    "    max_seq_len=MAX_TOKEN_LENGTH,\n",
    "    segment_vocab_size=WINDOWS_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    ff_hidden_dim=FF_HIDDEN_DIM,\n",
    "    k=K,\n",
    "    num_classes=2,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device, weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Model size: {total_params * 4 / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46c8225f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RUNNING INFERENCE ON TEST SET\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 8903/8903 [00:18<00:00, 475.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run inference on test set\n",
    "def evaluate_test_set(model, test_loader, device):\n",
    "    \"\"\"Evaluate the model on the test set and return predictions.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Running Inference\"):\n",
    "            input_ids, segment_ids, attention_masks, labels = [b.to(device) for b in batch]\n",
    "            logits = model(input_ids, segment_ids, attention_masks)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = logits.argmax(dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    return np.array(all_preds), np.array(all_labels), np.array(all_probs)\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RUNNING INFERENCE ON TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "predictions, labels, probabilities = evaluate_test_set(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50b6383b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLASSIFICATION REPORT\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal    0.99979   0.99985   0.99982     65366\n",
      "   Anomalous    0.99829   0.99761   0.99795      5854\n",
      "\n",
      "    accuracy                        0.99966     71220\n",
      "   macro avg    0.99904   0.99873   0.99888     71220\n",
      "weighted avg    0.99966   0.99966   0.99966     71220\n",
      "\n",
      "\n",
      "============================================================\n",
      "CONFUSION MATRIX\n",
      "============================================================\n",
      "           Pred_Normal  Pred_Anomalous\n",
      "Normal           65356              10\n",
      "Anomalous           14            5840\n",
      "\n",
      "============================================================\n",
      "SUMMARY METRICS\n",
      "============================================================\n",
      "Accuracy: 0.9997\n",
      "True Positives (Anomalies detected): 5840\n",
      "True Negatives (Normal correctly identified): 65356\n",
      "False Positives (False alarms): 10\n",
      "False Negatives (Missed anomalies): 14\n"
     ]
    }
   ],
   "source": [
    "# Generate and display results\n",
    "target_names = [\"Normal\", \"Anomalous\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(labels, predictions, target_names=target_names, digits=5))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\"*60)\n",
    "cm = confusion_matrix(labels, predictions)\n",
    "cm_df = pd.DataFrame(cm, index=target_names, columns=[f\"Pred_{n}\" for n in target_names])\n",
    "print(cm_df)\n",
    "\n",
    "# Calculate key metrics\n",
    "accuracy = (predictions == labels).mean()\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"True Positives (Anomalies detected): {tp}\")\n",
    "print(f\"True Negatives (Normal correctly identified): {tn}\")\n",
    "print(f\"False Positives (False alarms): {fp}\")\n",
    "print(f\"False Negatives (Missed anomalies): {fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29e9444f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Screener function ready!\n",
      "Usage: result = screen_logs(log_messages_list, model, tokenizer, device)\n"
     ]
    }
   ],
   "source": [
    "# Screener function for new log sessions\n",
    "def screen_logs(log_messages, model, tokenizer, device, windows_size=10):\n",
    "    \"\"\"\n",
    "    Screen a sequence of log messages for anomalies.\n",
    "    \n",
    "    Args:\n",
    "        log_messages: List of log message strings\n",
    "        model: Trained AllLinLog model\n",
    "        tokenizer: GPT-4 tokenizer\n",
    "        device: torch device\n",
    "        windows_size: Number of logs per session\n",
    "    \n",
    "    Returns:\n",
    "        dict with prediction, probability, and confidence\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Ensure we have enough logs\n",
    "    if len(log_messages) < windows_size:\n",
    "        # Pad with empty strings if needed\n",
    "        log_messages = log_messages + [\"\"] * (windows_size - len(log_messages))\n",
    "    \n",
    "    # Take only windows_size logs\n",
    "    log_messages = log_messages[:windows_size]\n",
    "    \n",
    "    # Tokenize\n",
    "    input_ids, segment_ids = tokenize_and_construct_input(log_messages, tokenizer, MAX_TOKEN_LENGTH)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    input_ids_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "    segment_ids_tensor = torch.tensor([segment_ids], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Pad to max length\n",
    "    if input_ids_tensor.size(1) < MAX_TOKEN_LENGTH:\n",
    "        pad_size = MAX_TOKEN_LENGTH - input_ids_tensor.size(1)\n",
    "        input_ids_tensor = torch.nn.functional.pad(input_ids_tensor, (0, pad_size), value=0)\n",
    "        segment_ids_tensor = torch.nn.functional.pad(segment_ids_tensor, (0, pad_size), value=0)\n",
    "    \n",
    "    attention_mask = (input_ids_tensor != 0).long()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids_tensor, segment_ids_tensor, attention_mask)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        pred = logits.argmax(dim=1).item()\n",
    "    \n",
    "    return {\n",
    "        \"prediction\": \"Anomalous\" if pred == 1 else \"Normal\",\n",
    "        \"anomaly_probability\": probs[0, 1].item(),\n",
    "        \"normal_probability\": probs[0, 0].item(),\n",
    "        \"confidence\": probs[0, pred].item()\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage\n",
    "print(\"\\nScreener function ready!\")\n",
    "print(\"Usage: result = screen_logs(log_messages_list, model, tokenizer, device)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b904bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DEMO: Screening sample sessions from test set\n",
      "============================================================\n",
      "\n",
      "Sample 1: Actual=Normal, Predicted=Normal ✓\n",
      "  Anomaly probability: 0.0000\n",
      "\n",
      "Sample 2: Actual=Normal, Predicted=Normal ✓\n",
      "  Anomaly probability: 0.0000\n",
      "\n",
      "Sample 3: Actual=Anomalous, Predicted=Anomalous ✓\n",
      "  Anomaly probability: 1.0000\n",
      "\n",
      "Sample 4: Actual=Anomalous, Predicted=Anomalous ✓\n",
      "  Anomaly probability: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Demo: Screen a sample from the test set\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEMO: Screening sample sessions from test set\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get a few samples (normal and anomalous)\n",
    "normal_samples = [s for s in test_sessions if s[\"session_label\"] == 0][:2]\n",
    "anomalous_samples = [s for s in test_sessions if s[\"session_label\"] == 1][:2]\n",
    "\n",
    "for i, sample in enumerate(normal_samples + anomalous_samples):\n",
    "    actual = \"Normal\" if sample[\"session_label\"] == 0 else \"Anomalous\"\n",
    "    \n",
    "    # Decode tokens back to text (simplified - just for demo)\n",
    "    decoded = tokenizer.decode(sample[\"input_ids\"])\n",
    "    \n",
    "    # Re-run through screener\n",
    "    input_ids = torch.tensor([sample[\"input_ids\"]], dtype=torch.long).to(device)\n",
    "    segment_ids = torch.tensor([sample[\"segment_ids\"]], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Pad if needed\n",
    "    if input_ids.size(1) < MAX_TOKEN_LENGTH:\n",
    "        pad_size = MAX_TOKEN_LENGTH - input_ids.size(1)\n",
    "        input_ids = torch.nn.functional.pad(input_ids, (0, pad_size), value=0)\n",
    "        segment_ids = torch.nn.functional.pad(segment_ids, (0, pad_size), value=0)\n",
    "    \n",
    "    attention_mask = (input_ids != 0).long()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, segment_ids, attention_mask)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        pred = \"Anomalous\" if logits.argmax(dim=1).item() == 1 else \"Normal\"\n",
    "    \n",
    "    status = \"✓\" if pred == actual else \"✗\"\n",
    "    print(f\"\\nSample {i+1}: Actual={actual}, Predicted={pred} {status}\")\n",
    "    print(f\"  Anomaly probability: {probs[0, 1].item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
