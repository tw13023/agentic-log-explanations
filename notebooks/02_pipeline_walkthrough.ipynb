{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e12341cb",
   "metadata": {},
   "source": [
    "# Screener-Reasoner Pipeline Walkthrough\n",
    "\n",
    "**Date**: 2026-01-30\n",
    "\n",
    "This notebook explains the complete pipeline for explainable log anomaly detection.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "┌─────────────┐     ┌─────────────┐     ┌─────────────┐     ┌─────────────┐\n",
    "│  Log Data   │ ──▶ │  Screener   │ ──▶ │  Retriever  │ ──▶ │  Reasoner   │\n",
    "│  (BGL/HDFS) │     │ (AllLinLog) │     │   (BM25)    │     │   (LLM)     │\n",
    "└─────────────┘     └─────────────┘     └─────────────┘     └─────────────┘\n",
    "                           │                   │                   │\n",
    "                           ▼                   ▼                   ▼\n",
    "                    Anomaly Detection    Evidence Docs    Structured Explanation\n",
    "```\n",
    "\n",
    "## What We Built Today\n",
    "\n",
    "| Module | Purpose |\n",
    "|--------|--------|\n",
    "| `data_loader.py` | Load BGL/HDFS datasets into Session objects |\n",
    "| `screener.py` | AllLinLog model wrapper for anomaly detection |\n",
    "| `evidence_store.py` | Build evidence corpus from training data |\n",
    "| `retriever.py` | BM25 retrieval for RAG |\n",
    "| `llm_client.py` | Unified LLM client (Ollama/OpenAI) |\n",
    "| `prompt_builder.py` | Build structured prompts for explanation |\n",
    "| `verifier.py` | Verify claims against evidence |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc1333c",
   "metadata": {},
   "source": [
    "## Step 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c2ffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "# Import all modules\n",
    "from src import (\n",
    "    BGLDataLoader, Session,\n",
    "    Screener, ScreenerOutput,\n",
    "    EvidenceStore,\n",
    "    BM25Retriever,\n",
    "    PromptBuilder, TraceExplanation,\n",
    "    LLMClient,\n",
    ")\n",
    "\n",
    "print(\"✓ All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9a7420",
   "metadata": {},
   "source": [
    "## Step 1: Load Data\n",
    "\n",
    "The `BGLDataLoader` reads BGL log files and creates `Session` objects using a sliding window approach.\n",
    "\n",
    "- **Window size**: 10 log lines per session\n",
    "- **Label**: If ANY line in the window is anomalous, the session is labeled as anomaly\n",
    "- **Split**: 70% train, 15% validation, 15% test (stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34f0cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BGL dataset\n",
    "loader = BGLDataLoader('../logs/BGL.log', windows_size=10)\n",
    "loader.load()\n",
    "\n",
    "# Get splits\n",
    "train_sessions = loader.get_sessions(split='train')\n",
    "test_sessions = loader.get_sessions(split='test')\n",
    "\n",
    "print(f\"Train sessions: {len(train_sessions):,}\")\n",
    "print(f\"Test sessions:  {len(test_sessions):,}\")\n",
    "\n",
    "# Check label distribution\n",
    "train_anomaly = sum(1 for s in train_sessions if s.label == 1)\n",
    "test_anomaly = sum(1 for s in test_sessions if s.label == 1)\n",
    "print(f\"\\nTrain anomaly rate: {train_anomaly/len(train_sessions):.1%}\")\n",
    "print(f\"Test anomaly rate:  {test_anomaly/len(test_sessions):.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6c6914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a sample session\n",
    "sample = test_sessions[0]\n",
    "print(f\"Session ID: {sample.session_id}\")\n",
    "print(f\"Label: {'Anomaly' if sample.label == 1 else 'Normal'}\")\n",
    "print(f\"Number of lines: {len(sample.lines)}\")\n",
    "print(f\"\\nLog content:\")\n",
    "for i, line in enumerate(sample.lines[:5]):\n",
    "    print(f\"  {i+1}. {line[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99031e2b",
   "metadata": {},
   "source": [
    "## Step 2: Screener (Anomaly Detection)\n",
    "\n",
    "The `Screener` wraps the pre-trained **AllLinLog** model:\n",
    "- **Architecture**: Linformer (linear attention) for efficient long sequence processing\n",
    "- **Tokenizer**: GPT-4 BPE (cl100k_base)\n",
    "- **Output**: Binary classification (normal/anomaly) with probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f139e22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained Screener model\n",
    "screener = Screener(\n",
    "    model_path='../best_model/best_model_20250724_072857.pth',\n",
    "    dataset='BGL'\n",
    ")\n",
    "screener.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2659004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a few sessions\n",
    "print(\"Testing Screener predictions:\\n\")\n",
    "\n",
    "# Get some anomaly and normal sessions\n",
    "anomaly_sessions = [s for s in test_sessions if s.label == 1][:3]\n",
    "normal_sessions = [s for s in test_sessions if s.label == 0][:3]\n",
    "\n",
    "for session in anomaly_sessions + normal_sessions:\n",
    "    result = screener.predict(session)\n",
    "    actual = \"Anomaly\" if session.label == 1 else \"Normal\"\n",
    "    predicted = \"Anomaly\" if result.pred == 1 else \"Normal\"\n",
    "    match = \"✓\" if session.label == result.pred else \"✗\"\n",
    "    print(f\"{session.session_id}: Actual={actual:7}, Pred={predicted:7}, Prob={result.prob[1]:.4f} {match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec88c49",
   "metadata": {},
   "source": [
    "## Step 3: Evidence Store & Retriever (RAG)\n",
    "\n",
    "For explainability, we use RAG (Retrieval-Augmented Generation):\n",
    "\n",
    "1. **Evidence Store**: Build a corpus from training sessions\n",
    "2. **BM25 Retriever**: Find similar sessions as evidence for explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929660d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build evidence store from training data\n",
    "# Using a subset for speed (in production, use all training data)\n",
    "evidence_store = EvidenceStore(dataset='BGL')\n",
    "evidence_store.build_from_sessions(train_sessions[:3000])\n",
    "\n",
    "print(f\"Evidence store size: {len(evidence_store)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0efa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build BM25 retriever\n",
    "retriever = BM25Retriever(evidence_store=evidence_store)\n",
    "retriever.build_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd243d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval for an anomaly session\n",
    "query_session = anomaly_sessions[0]\n",
    "print(f\"Query session: {query_session.session_id}\")\n",
    "print(f\"Query content: {query_session.lines[0][:80]}...\\n\")\n",
    "\n",
    "# Retrieve similar evidence\n",
    "hits = retriever.retrieve_for_session(query_session, top_k=3)\n",
    "\n",
    "print(\"Top 3 retrieved evidence:\")\n",
    "for i, hit in enumerate(hits, 1):\n",
    "    print(f\"\\n{i}. Score: {hit.score:.4f}\")\n",
    "    print(f\"   ID: {hit.evidence_id}\")\n",
    "    print(f\"   Text: {hit.text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c79be6",
   "metadata": {},
   "source": [
    "## Step 4: Prompt Builder\n",
    "\n",
    "The `PromptBuilder` creates structured prompts for the LLM:\n",
    "- Includes the anomalous session content\n",
    "- Includes retrieved evidence with IDs (E1, E2, ...)\n",
    "- Requests JSON output with traceable claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bfacda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build prompt for explanation\n",
    "builder = PromptBuilder()\n",
    "screener_output = screener.predict(query_session)\n",
    "\n",
    "system_prompt, user_prompt = builder.build_prompt(\n",
    "    session=query_session,\n",
    "    screener_output=screener_output,\n",
    "    evidence_hits=hits\n",
    ")\n",
    "\n",
    "print(\"=== SYSTEM PROMPT ===\")\n",
    "print(system_prompt[:500])\n",
    "print(\"\\n=== USER PROMPT (first 1500 chars) ===\")\n",
    "print(user_prompt[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831c135e",
   "metadata": {},
   "source": [
    "## Step 5: LLM Reasoner\n",
    "\n",
    "The `LLMClient` supports multiple providers with a unified interface:\n",
    "\n",
    "| Provider | Model | Cost |\n",
    "|----------|-------|------|\n",
    "| `ollama` | llama3.1:8b | Free (local) |\n",
    "| `openai` | gpt-4o | ~$0.007/explanation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173f883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test both LLM providers\n",
    "print(\"Available LLM providers:\\n\")\n",
    "\n",
    "# Ollama (local)\n",
    "ollama_client = LLMClient(provider=\"ollama\", model=\"llama3.1:8b\")\n",
    "print(f\"Ollama available: {ollama_client.is_available()}\")\n",
    "print(f\"  Models: {ollama_client.list_models()}\")\n",
    "\n",
    "# OpenAI (cloud)\n",
    "openai_client = LLMClient(provider=\"openai\", model=\"gpt-4o\")\n",
    "print(f\"\\nOpenAI available: {openai_client.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eb23d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate explanation with Ollama (local, free)\n",
    "print(\"Generating explanation with Ollama (llama3.1:8b)...\\n\")\n",
    "\n",
    "response = ollama_client.generate(\n",
    "    prompt=user_prompt,\n",
    "    system_prompt=system_prompt,\n",
    "    json_mode=True,\n",
    "    temperature=0.1,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "print(f\"Latency: {response.latency_ms:.0f}ms\")\n",
    "print(f\"Tokens: {response.total_tokens}\")\n",
    "print(f\"\\n=== EXPLANATION ===\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ab11b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate explanation with OpenAI (cloud, paid)\n",
    "print(\"Generating explanation with OpenAI (gpt-4o)...\\n\")\n",
    "\n",
    "response = openai_client.generate(\n",
    "    prompt=user_prompt,\n",
    "    system_prompt=system_prompt,\n",
    "    json_mode=True,\n",
    "    temperature=0.1,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "print(f\"Latency: {response.latency_ms:.0f}ms\")\n",
    "print(f\"Tokens: {response.total_tokens}\")\n",
    "print(f\"Cost: ${response.cost_usd:.4f}\")\n",
    "print(f\"\\n=== EXPLANATION ===\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605f3d80",
   "metadata": {},
   "source": [
    "## Step 6: Parse Structured Explanation\n",
    "\n",
    "The LLM returns a JSON object with:\n",
    "- `prediction`: \"anomaly\" or \"normal\"\n",
    "- `summary`: Brief explanation\n",
    "- `claims`: List of claims, each with `evidence_ids`\n",
    "- `insufficient_evidence`: Boolean flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3637be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Parse the JSON response\n",
    "explanation = json.loads(response.content)\n",
    "\n",
    "print(f\"Prediction: {explanation['prediction']}\")\n",
    "print(f\"\\nSummary: {explanation['summary']}\")\n",
    "print(f\"\\nClaims:\")\n",
    "for i, claim in enumerate(explanation['claims'], 1):\n",
    "    print(f\"  {i}. {claim['claim']}\")\n",
    "    print(f\"     Evidence: {claim['evidence_ids']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfceffc5",
   "metadata": {},
   "source": [
    "## Complete Pipeline Function\n",
    "\n",
    "Here's the complete pipeline wrapped in a single function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8fdc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_anomaly(\n",
    "    session: Session,\n",
    "    screener: Screener,\n",
    "    retriever: BM25Retriever,\n",
    "    llm_client: LLMClient,\n",
    "    top_k: int = 5\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Complete pipeline to explain a log anomaly.\n",
    "    \n",
    "    Args:\n",
    "        session: Session to explain\n",
    "        screener: Loaded Screener model\n",
    "        retriever: Fitted BM25 retriever\n",
    "        llm_client: LLM client (Ollama or OpenAI)\n",
    "        top_k: Number of evidence documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Dict with prediction, explanation, and metadata\n",
    "    \"\"\"\n",
    "    # Step 1: Screen for anomaly\n",
    "    screener_output = screener.predict(session)\n",
    "    \n",
    "    if screener_output.pred == 0:\n",
    "        return {\n",
    "            \"session_id\": session.session_id,\n",
    "            \"prediction\": \"normal\",\n",
    "            \"explanation\": \"Session classified as normal by the screener.\",\n",
    "            \"screener_prob\": screener_output.prob[1]\n",
    "        }\n",
    "    \n",
    "    # Step 2: Retrieve evidence\n",
    "    hits = retriever.retrieve_for_session(session, top_k=top_k)\n",
    "    \n",
    "    # Step 3: Build prompt\n",
    "    builder = PromptBuilder()\n",
    "    system_prompt, user_prompt = builder.build_prompt(\n",
    "        session=session,\n",
    "        screener_output=screener_output,\n",
    "        evidence_hits=hits\n",
    "    )\n",
    "    \n",
    "    # Step 4: Generate explanation\n",
    "    response = llm_client.generate(\n",
    "        prompt=user_prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        json_mode=True\n",
    "    )\n",
    "    \n",
    "    # Step 5: Parse and return\n",
    "    explanation = json.loads(response.content)\n",
    "    \n",
    "    return {\n",
    "        \"session_id\": session.session_id,\n",
    "        \"screener_prob\": screener_output.prob[1],\n",
    "        \"explanation\": explanation,\n",
    "        \"evidence_count\": len(hits),\n",
    "        \"llm_latency_ms\": response.latency_ms,\n",
    "        \"llm_cost_usd\": response.cost_usd\n",
    "    }\n",
    "\n",
    "print(\"✓ Pipeline function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd667e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the complete pipeline\n",
    "result = explain_anomaly(\n",
    "    session=anomaly_sessions[0],\n",
    "    screener=screener,\n",
    "    retriever=retriever,\n",
    "    llm_client=ollama_client  # Use local LLM\n",
    ")\n",
    "\n",
    "print(f\"Session: {result['session_id']}\")\n",
    "print(f\"Screener probability: {result['screener_prob']:.4f}\")\n",
    "print(f\"Evidence retrieved: {result['evidence_count']}\")\n",
    "print(f\"LLM latency: {result['llm_latency_ms']:.0f}ms\")\n",
    "print(f\"\\nExplanation:\")\n",
    "print(json.dumps(result['explanation'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e07df65",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Built Today\n",
    "\n",
    "1. **Data Loading** (`BGLDataLoader`)\n",
    "   - Sliding window session creation\n",
    "   - Stratified train/val/test split\n",
    "\n",
    "2. **Anomaly Detection** (`Screener`)\n",
    "   - AllLinLog model wrapper\n",
    "   - GPT-4 tokenizer\n",
    "   - Binary classification with probability\n",
    "\n",
    "3. **RAG Pipeline**\n",
    "   - `EvidenceStore`: Build corpus from training data\n",
    "   - `BM25Retriever`: Retrieve similar sessions\n",
    "\n",
    "4. **LLM Explanation** (`LLMClient`)\n",
    "   - Unified interface for Ollama/OpenAI\n",
    "   - Structured JSON output with traceable claims\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- [ ] Implement `Verifier` to check claim faithfulness\n",
    "- [ ] Test on HDFS dataset\n",
    "- [ ] Batch processing pipeline\n",
    "- [ ] Evaluation metrics (faithfulness, coverage)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
